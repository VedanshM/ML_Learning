{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains function related to the logictic regression.  \n",
    "This contains a function <name> which accepts train set and generates x and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X):\n",
    "    rwise_mean = np.nanmean(X, axis = 1, keepdims=True)\n",
    "    rwise_std = np.nanstd(X, axis=1, keepdims=True)\n",
    "    rwise_std[rwise_std == 0] = 1\n",
    "    return (X - rwise_mean)/(rwise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1+ np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wb(dim):\n",
    "    w = np.random.randn(dim,1)\n",
    "    # w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunc(A,Y):\n",
    "    m = Y.shape[1]\n",
    "    # assert(not np.any( A*(1-A) ==0)), \"A cant have 0 and 1\"\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(X,Y,w,b):\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) +b)\n",
    "    cost =costFunc(A,Y)\n",
    "    #calculating  gradients now\n",
    "    grads = {}\n",
    "    grads[\"dw\"] = (1/m)*np.dot(X, (A-Y).T)\n",
    "    grads[\"db\"] = (1/m)*np.sum(A-Y)\n",
    "    assert(grads[\"db\"] == grads[\"db\"])\n",
    "    #asserts for debugging\n",
    "    assert(grads[\"dw\"].shape == w.shape)\n",
    "    assert(grads[\"db\"].dtype == float)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return grads,cost\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionRun(w, b, X, Y, num_iterations, learning_rate, print_cost = True):\n",
    "    \"\"\"\n",
    "    Optimizes w and b by running gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w : intial weights, a numpy array of size (nx, 1)\n",
    "    b : initial bias, a scalar\n",
    "    X : data of shape (nx, number of examples)\n",
    "    Y : boolean label vector\n",
    "    num_iterations : number of iterations of the optimization loop\n",
    "    learning_rate : learning rate of the gradient descent update rule\n",
    "    print_cost : True to print the loss every 100 steps\n",
    "    \n",
    "    Returns: params, grads, costs\n",
    "    > params : dictionary containing the weights w and bias b\n",
    "    > grads : dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    > costs : list of costs (every 100 iter) computed during the optimization, be used to plot the learning curve.\n",
    "        \"\"\"\n",
    "    costs = []\n",
    "    grads = {}\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(X,Y,w,b)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w -= learning_rate*dw\n",
    "        b -= learning_rate*db\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            costs.append(cost)\n",
    "            if(print_cost):\n",
    "                print(\"Cost after iteration {0}: {1}\".format(i,cost))\n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    return params, grads, costs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w,b):\n",
    "    \"\"\"\n",
    "        Predicts Y vector given w,b parameters from data set X\n",
    "        X: np.array of shape (nx, m)\n",
    "        w: np.array of shape (1,nx)\n",
    "        b: real num\n",
    "        Y: np.array of shape (1,m)\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) +b)\n",
    "    print(A)\n",
    "    Y_pred = np.zeros((1,m))\n",
    "    assert(A.shape == Y_pred.shape)\n",
    "    for i in range(m):\n",
    "        Y_pred[0][i] = 1 if(A[0][i] > 0.5) else 0\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = True):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the functions implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train : training set represented by a numpy array of shape (nx, m_train)\n",
    "    Y_train : training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test : test set represented by a numpy array of shape (nx, m_test)\n",
    "    Y_test : test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations : hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate : hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost : Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns: d : dictionary containing information about the model, i.e \n",
    "        \"costs\",\n",
    "        \"Y_prediction_test\" \n",
    "        \"Y_prediction_train\" \n",
    "        \"w\" : w, \n",
    "        \"b\" : b,\n",
    "        \"learning_rate\" : learning_rate,\n",
    "        \"num_iterations\"\n",
    "    \"\"\"\n",
    "    m = X_train.shape[1]\n",
    "    nx = X_train.shape[0]\n",
    "    X_train = optimize(X_train)\n",
    "    X_test = optimize(X_test)\n",
    "    w,b = initialize_wb(nx)\n",
    "    params, grads, costs = regressionRun(w,b,X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    Y_prediction_train = predict(X_train, w, b)\n",
    "    Y_prediction_test = predict(X_test, w, b)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    d = {\n",
    "        \"Y_prediction_train\" : Y_prediction_train,\n",
    "        \"Y_prediction_test\" : Y_prediction_test,\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"learning_rate\" : learning_rate,\n",
    "        \"num_iterations\" : num_iterations\n",
    "    }\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
