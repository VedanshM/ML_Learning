{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of optimizing a NN is that we get an idea, code, experiment and repeat thus eventually getting an optimized, working NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "It is equivalent to aiming at wrong place and means that we have underfitting. We hope for low bias. Possible solution for High bias:\n",
    "- Bigger netwrok.\n",
    "- train longer.\n",
    "       \n",
    "#### Variance\n",
    "Equivalent of having unsteady aim. Usually occurs due to overfitting but if NN is too bad then we may have both high bias and high variance which means nothing works well.  \n",
    "Possible solutions for high variance:\n",
    "- Get more data.\n",
    "- regularizaion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimal error (Bayes error): \n",
    "The expected error from an ideal NN (think of an human's ability) to solve the task. Helps in deciding how much error is OK for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the data\n",
    "So we can divide that data in three partitions namely training, dev and test. \n",
    "Generally divide the data set in 60:20:20 ratio but if the data size is too large , e.g. we have say 1e6 examples in our data and may be 10k (1%) may be enough for dev and test set.\n",
    "> All data should come from single source otherwise things will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing data\n",
    "We should normalize our data to make cost function less elongated(imagine in 2D) and thus when we apply gradient decent we follow the path more linearly than moving zigzag. Normalizing the data mathematically mean :  \n",
    "   $$ x = \\frac{x-\\mu}{\\sigma} $$  \n",
    " where $\\mu$ is the mean of $x$ and $\\sigma$ is the standard mean.\n",
    "> Take care of the case for st. mean equal to zero. Then u can just zero out the whole feature to remove it from calculation because anyway it is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vanishing/ Exploding gradients\n",
    "In case of very deep NN, the problem may arise that the function may increase or shrink dramatically,  suppose in case of linear act function, see yhat as the product of (W) with X  ( let b=0).\n",
    "Now if L is large the we have yhat = W^L X (may be not exactly ) and thus large L may cause to value of yhat to explode if W is >1 and to shrink if W <1 . All this makes training difficult , eg if W<1 and thus yhat dec, thus gradient also shrink and gradient decent take tiny steps.  \n",
    "\n",
    "> P.S. make initializations smartly to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight  Initialization for Deep Networks\n",
    "Here are some tips to initialize the weights of NN, we keep the mean zero and choose variance acc to the actv func. this data is generated by using np.random.randn() which gives zero mean and unit variance, multiply the result with desired std deviation.\n",
    "- for tanh use Xavier's initialization:  \n",
    "$$\\mu = 0  ,   \\sigma = \\sqrt{\\frac{1}{{n^{[l]}}}} $$\n",
    "    \n",
    "- for Relu use He initialization :  \n",
    "$$\\mu = 0  ,   \\sigma = \\sqrt{\\frac{2}{{n^{[l-1]}}}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization  \n",
    "####  L2 regularization  \n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "\n",
    "Now why this helps because when we try to minimize the cost we also minimize the added term ans hence if lambda is large  then we have to make Ws small. now making Ws small we have an effect of reducing the contributions of neurons and see it as we will cancel/ remove some neurons from our network and thus will make it more simpler, and thus we can reduce Overfitting.  \n",
    "\n",
    "Another intuition regarding this can be from that when we use tanh function, reducing Ws will make Z closerto zero where our function( tanh) is almost linear and thus acts as linear activation function. remember that linear activation function makes NN effectless(because it makes the function calculated by the NN simple and not very complex) and thus this also reduces the overfitting done.\n",
    "\n",
    "### Dropout regularization\n",
    "It is a widely used regularization technique that is specific to deep learning. \n",
    "**It randomly shuts down some neurons in each iteration.** To make sure the expected value of A is same then we multiply A by 1/keep_prob. Now since a neuron cant rely on any one particular feature for its value it have to disctibute its weight among other features and thus reducing the sum of squared elements of W.\n",
    "\n",
    "##### Other regularization techniques  \n",
    "- Data augmentation : In this we try to get more data from out already present dataset. eg in case of images we can flip images, or rotate with a acute angle to generate new images for training.\n",
    "- Early stopping: What we do in this case is that plot graph between train error and no. of iterations and along with and train error we also plot graph for our dev set error. The train set error should always be decreasing but is we find that the dev set has attained a minimum and then started rising again we can stop out training at that iterations. Another intuition is that when we start training the W has small values which turn bigger, due to large number of iterations they may become too large and thus we need to stop early. Downside of this is that we try to optimize J and also make sure not to overfit in a single task, and early stopping may not allow us to minimize J properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical aproax of gradients\n",
    "we can check the calc our gradients by comparing it with the one calculated using the apporx method by making tiny triangle in a graph. Use this only in debug. Doesnt work in dropout( if want to use turn off drop out , check gra, turn dropout back on).  \n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and your loss function.\n",
    "\n",
    "Because forward propagation is relatively easy to implement, you're confident you got that right, and so you're almost  100% sure that you're computing the cost $J$ correctly. Thus, you can use your code for computing $J$ to verify the code for computing $\\frac{\\partial J}{\\partial \\theta}$. \n",
    "\n",
    "Let's look back at the definition of a derivative (or gradient):\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$  \n",
    "**Instructions**:\n",
    "- First compute \"gradapprox\" using the formula above (1) and a small value of $\\varepsilon$. Here are the Steps to follow:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- Then compute the gradient using backward propagation, and store the result in a variable \"grad\"\n",
    "- Finally, compute the relative difference between \"gradapprox\" and the \"grad\" using the following formula:\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "You will need 3 Steps to compute this formula:\n",
    "   - 1'. compute the numerator using np.linalg.norm(...)\n",
    "   - 2'. compute the denominator. You will need to call np.linalg.norm(...) twice.\n",
    "   - 3'. divide them.\n",
    "- If this difference is small (say less than $10^{-7}$), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch Gradient decent\n",
    "(Simple gradient decent is called batch gradient decent)  \n",
    "This technique is used to spped up the NN because now we will be processing a smaller size of matrix by taking a fewer samples in a \"mini batch\" (usually the size is 64, 128, 256).  \n",
    "There are two steps to divide the sample data into mini batches:   \n",
    "+ **Shuffle**: Create a shuffled version of the training set (X, Y). It is done synchronously between X and Y.\n",
    "+ **Partition**: Partition the shuffled (X, Y) into mini-batches of size selected. The last  batch may be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponentially Weighted average\n",
    "(need update)\n",
    "Set v0 as 0 and then use vi = (beta)(vi) + (1-beta) xi for averaging over variable x.\n",
    "(1)/(1 - beta) is the practially number of elements being averaged.  \n",
    "**Bias correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n",
    "\n",
    "Momentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable vv. Formally, this will be the exponentially weighted average of the gradient on previous steps.  \n",
    "This means to make back prop with this:\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "> Here $\\beta$ (lies in $(0,1)$) is another hyper parameter which controls how much momentum due we want that our path should have. Mathematically, the higher its value the more number of gradients are being  averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n",
    "\n",
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. Their usual values are 0.9 and 0.999 resp.\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero. Mostly 1e-8.\n",
    "\n",
    "Adam converges a lot faster.\n",
    "\n",
    "Some advantages of Adam include:\n",
    "- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n",
    "- Usually works well even with little tuning of hyperparameters (except $\\alpha$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learing rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order/ importance of tuning a particular hyperparameter  \n",
    "Here is a general list of hyperparameters associated with a NN.(in order of tuning imp.)  \n",
    "+ $\\alpha$ (learning rate most important for tuning).  \n",
    "+ $\\beta$ (for momentum, lies in (0,1)).  \n",
    "+ *miniBatch Size* ( gernerally 64,128,256..)\n",
    "+ *#HiddenUnits* \n",
    "+ *#HiddenLayers*\n",
    "+ *learning Rate Decay*\n",
    "+ *Adam Parameters* ($\\beta_1, \\beta_2, \\epsilon$, generally not tuned and equal to 0.9, 0.999, 1e-8 resp.)\n",
    "\n",
    "##### Try Random\n",
    "Suppose at a time we are tuning 2 paramenters so its like we have a 2D plane as a sample space and we would like to have some points in grid to have brite force over them but this method is not good because one parameter may not be effecting our NN as much as the other parameter. Like we have $\\alpha$ and eplison for tuning, so we'll see that changing eps doesnt count as much. hence use randm points in the plane and thus we have all points with different values of aplha and thus we can check over more values over it.   \n",
    "\n",
    "##### Have proper scale\n",
    "Some parameters like alpha and beta are not to be tuned with samples randomly distributed on a linear scale, eg if alpha lies in 0.0001 and 1, random in linear scale would produce 90% of the results in range 0.1 and 1. Thus distribute over log scale by picking exp of 0.1 randomly. Similar write beta as 1-x form and generate diff x as said above.\n",
    "\n",
    "##### Tuning in practice\n",
    "Either  we can do tuning while parts of NN is trained by observing the graphs (pandas method) or try diff sets of NN at a time and see at the end which worked well. The method generally depends on the computation power available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalizations \n",
    "We can normalize the $Z$ produced in each layer to make our NN perform better. Firstly we make the mean and stddev to 0 and 1 resp. and then introduce two new parameters $\\gamma$ and $\\beta$ (not the beta from momentm), which denote new sttdev and mean resp.\n",
    "> *Note*:  \n",
    "+ Since we are normalizing, we now dont need a $b$ for our NN and now $z = w a^{[l-1]}$ can be the formula for z and then it is normalized. Hence set $b$ to zero permanent.\n",
    "+ The parameters gamma and beta are learnable as $W$ is.\n",
    "+ works with GD with momentum, RMSprop, Adam too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Markdown",
   "language": "markdown",
   "name": "markdown"
  },
  "language_info": {
   "codemirror_mode": "markdown",
   "file_extension": ".md",
   "mimetype": "text/markdown",
   "name": "Markdown",
   "pygments_lexer": "markdown"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
