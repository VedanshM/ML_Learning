{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since the the simple logistic regression can only classify things by dividing the sample space linearly, we need some more complex ways to classify things. \n",
    ">The coursera course mainly talked about the binary classificaion hence other functions are yet to be discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we introduce some hidden layers between the input features anf the output.  \n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
    "\n",
    "$L$: no of hidden layers  \n",
    "        - by default input layer is layer 0 and output layer is layer L    \n",
    "$nh[i]$ : is number of hidden layer units in layer $i$\n",
    "$W^{[i]}$ : matrix of dim($nh[i-1], nh$)  \n",
    "$b^{[i]}$ : coulmn matrix of dim($nh[i], 1$)  \n",
    "$z^{[i]}$ : matrix obtained by $ z^{[i]} = W^{[i]} A^{[i-1]} + b^{[i]} $,  dim is $(nh[i],1)$  \n",
    "$g^{[i]}$ : activation function used for ith layer, for bin classification use sigmoid in last layer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some activation functions are:  \n",
    "   - sigmoid :  $\\sigma(x) = \\frac{1}{1 + e^{-x}} $. used for bin classi. not used in middle layers because not zero centered distribution.\n",
    "   - tanh : $ \\tanh(x) = \\frac{e^x - e{-x}}{e^x + e^{-x}} $. better than sigmoid as zero centred but both of them have nearly zero slope at large values , hence need  replacement.\n",
    "   - relu : $Relu(x) = max(0,x)$ Slope constant for +ve x. **Generally used**.\n",
    "         Some better variations of relu exist but not used generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematically**(for L=2, i.e. only single hidden layer):\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    2. Initialize the model's parameters\n",
    "    3. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)\n",
    "\n",
    "You often build helper functions to compute steps 1-3 and then merge them into one function we call `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining neural networks\n",
    "when deciding number of hidden units, consider that large number of units may overfit the data and thus use wisely.  \n",
    "Initialize nx, nh, ny for further usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization for W and b\n",
    "If we make W all zeroes then all units in a single layer act identically. Use np.random.randn() for W.\n",
    "Generally it doesnt make a diff for b to be random or not, hence used np.zeros() for it.  \n",
    "Make sure to give correct dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "loop over the hidden layers and calculate the required:\n",
    "    $$ Z^{[i]} = W^{[i]}A^{[i-1]}+ b^{[i]} $$\n",
    "    $$ A^{[i]} = g^{[i]}(Z^{[i]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost\n",
    "Take the final $A^{[L]}$ and $Y$ and compute the cost function  $J$. (this J is actually the average of the outputs of the loss function over all examples, hence the thing in bracket with the negative sign is out loss function here.)       \n",
    "  $$ J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right)  \\large  \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagation\n",
    "  > note shorthand used is  $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$.  \n",
    "  \n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} \\tag{10}$$  \n",
    "To calculate $dZ^{[l]}$ use $dA^{[l]}$ and $Z^{[l]}$( cached from forward prop):  \n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$\n",
    "\n",
    "Hence for all these calculations we would require $dA^{[L]}$ and it can be computed as follows ( for sigmoid function):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update\n",
    "Update parameters as\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model again and again will eventually lower down the value of cost function and thus our NN will be trained. Note that we should check for underfitting and over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Clasification\n",
    "We can use the following method for having a generall classification of C classes:  \n",
    "#### Softmax regression\n",
    "After calculating the Z for the final layer we calculate t as `t = exp(z)` and then use the following formula to obtain the last output  i.e. yhat for our NN. Note that now our last layee should contain C units.\n",
    "        $$ A^{[l]} = \\frac{e^{z^{[L}}}{sumOver(e^{z^{[L}})} $$\n",
    "This generates values within range (0,1).  \n",
    "  \n",
    "Now we need a new loss function which is similar to that in binary classification:  \n",
    "$$ L =  \\sum\\limits_{i = 1}^{C} \\large\\left(\\small y^{(i)}\\log\\left(a^{[L] (i)}\\right)   \\large  \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
