{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "CNN are an excellent way to work upon the images as it logically extract features out of it and also uses very few parameters per layer (than FC) which reduces the chance of overfitting due to the ability to form a very complex function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works in a way, that there is a sqaure(may be a rect) of weights which act as window which slides over our image (ususally with some stride or step). Now this will reduce the dimensions of the next image and also the edges of image contribute less in this method so we have a system of Zero padding which is basically a border of zeros around our image to give it extra size and hence we can now decide the dimensions of the next image/layer. Usually the window has odd size.  We generally have multipe layers of such filters/windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "The we have something called pooling, which has the purpose of reducing the dim of the layer and also provides the ability to not to be affected by slight change in the positon of certain features because for example in *max pooling* we take a window and generate an output with the value of maximum of the inputs to the window. The window slides similarly as in CNN and is associates with a stride. Traditionally *average pooling* was used but max pooling proves to be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are already some well tried arrangements of the CNN, pooling and FC layers and thus it is better to take insipiration from them and also (if possible) use pretrained weights.  \n",
    "Usually there are firstly a few sets of layers with CNN following pooling and then at last the layer is flattened and out old FC comes. We have one or two FCs and then final output.  \n",
    "Some of these architectures are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ### LeNet-5\n",
    "\n",
    "It is a very old architecture and originally used $tanh$ and average pooling. But this can be mordernized but still is suitable only for very easy datasets.  \n",
    "It has 3 (CNN + pooling) layers and then 2 FC layers (hence has 5 in it's name).\n",
    "> In this classical neural network architecture successfully used on MNIST handwritten digit recogniser patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet():\n",
    "    return keras.Sequential([\n",
    "                          layers.Input((28,28)),\n",
    "                          layers.Reshape((28,28,1)),\n",
    "                          layers.ZeroPadding2D((2,2)),\n",
    "                          layers.Conv2D(6, kernel_size=(5,5), activation='tanh'),\n",
    "                          layers.AveragePooling2D((2,2)),\n",
    "                          layers.Conv2D(16, kernel_size=(5,5), activation='tanh'),\n",
    "                          layers.AveragePooling2D((2,2)),\n",
    "                          layers.Flatten(),\n",
    "                          layers.Dense(120, activation='tanh'),\n",
    "                          layers.Dense(84, activation='tanh'),\n",
    "                          layers.Dense(10, activation=None)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Batch Normalization: * It is a layer to normalize the CNN layers depth-wise, which helps in learing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ### VGG\n",
    "It is relatively modern and used blocks of 2CNN + 1 Pooling. relu and maxpooling is used.\n",
    "Following is a sample code picked from a model currenlty under work(also reduced in size for dataset) and thus only provides the feel of architecture.  \n",
    "An excerpt from medium blog states:  \n",
    "> VGG-16 is a simpler architecture model, since its not using much hyper parameters. It always uses 3 x 3 filters with stride of 1 in convolution layer and uses SAME padding in pooling layers 2 x 2 with stride of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG():\n",
    "    model = keras.Sequential([\n",
    "                          layers.Input((32,32,3)),\n",
    "                          layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "                          layers.BatchNormalization(),\n",
    "                          layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "                          layers.BatchNormalization(),\n",
    "                          layers.MaxPool2D((2,2)),\n",
    "                          # layers.Dropout(0.2),\n",
    "                          layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "                          layers.BatchNormalization(),\n",
    "                          layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "                          layers.BatchNormalization(),\n",
    "                          layers.MaxPool2D((2,2)),\n",
    "                          # layers.Dropout(0.2),\n",
    "                          layers.Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "                          layers.BatchNormalization(),\n",
    "                          layers.Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "                          layers.BatchNormalization(),\n",
    "                          layers.MaxPool2D((2,2)),\n",
    "                          # layers.Dropout(0.3),\n",
    "                          layers.Flatten(),\n",
    "                          layers.Dense(1024, activation='relu', kernel_initializer='he_uniform', \n",
    "                                       kernel_regularizer=keras.regularizers.l1_l2()\n",
    "                                       ),\n",
    "                          # layers.Dropout(0.4),\n",
    "                          layers.Dense(512, activation='relu', kernel_initializer='he_uniform', \n",
    "                                       kernel_regularizer=keras.regularizers.l1_l2()\n",
    "                                       ),\n",
    "                          layers.Dense(10, activation=None)\n",
    "    ], name= 'model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ There are other nets too like AlexNet, ResNet, Inception framework(GoogLeNet), etc."
   ]
  }
 ]
}